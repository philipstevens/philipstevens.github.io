```python
!ls /content/gdrive/MyDrive/Coll
```

     1000GreatestFilms.xls
     1P0A3965.jpg
     1P0A3973.jpg
     20200430_172309.jpg
     21stCenturyFilms.gsheet
    'a154b413-793d-4b47-9b43-54c58edc270d (1).gdoc'
     a154b413-793d-4b47-9b43-54c58edc270d.gdoc
    'AcctSt_Aug23 (1).pdf'
     AcctSt_Aug23.pdf
     AcctSt_Jul23.pdf
     AcctSt_Jun23.pdf
     AcctSt_Nov23.pdf
     AcctSt_Oct23.pdf
     AcctSt_Sep23.pdf
     Agoda_Booking_ID_472016505_–_RECEIPT_enclosed.pdf
    'Ai technologies .gdoc'
    'Annual budget (1).gsheet'
    'Annual budget.gsheet'
     archive
    'Automatic Groups.pdf'
    'A very short Introduction.gdoc'
    'Bangkok Flat Hunt.gsheet'
    'Bangkok Monthly Budget.gsheet'
     Birthday.gsheet
    'Booking-Request-Form (1).docx'
    'Booking-Request-Form (1).gdoc'
    'Booking-Request-Form (1).odt'
     Booking-Request-Form.docx
     Budget.gsheet
     CAEN_Program_Book_Double_Page.pdf
    'Cambridge Expense.gsheet'
     Cancer.pdf
     character-strengths-and-virtues.gdoc
     character-strengths-and-virtues.pdf
    'Colab Notebooks'
    'Conservation X Labs.gdoc'
    'Copy of Outlive Tracker Template.gsheet'
    'Copy of The Best Restaurants in Bangkok.gmap'
     cover_letter.gdoc
     crop_model2.keras
     crop_model3.keras
     crop_model4.keras
     crop_model.keras
    'cv_phil (1).pdf'
    'cv_phil (2).pdf'
    'cv_phil (3).pdf'
    'cv_phil (4).pdf'
     cv_phil.pdf
     cv-phil.zip
    'Dive Rhianna Jackson.PDF'
     DSC_3267.jpg
     en_crop_model4.keras
     fada.pdf
     Finances.gsheet
     Food.gsheet
     Games.gdoc
     Goals.gdoc
    'Goal Tracking.gsheet'
     GOPR1873~2.mp4
     GOPR1874~2.mp4
     GOPR1874.MP4
     greatest_films_2023.gsheet
     greatest-films-ever-made.gsheet
    'Greatest Films.gsheet'
    'greatest-films-of-all-time-and-21st-century-1 (1).gsheet'
     greatest-films-of-all-time-and-21st-century-1.gsheet
    'GymFlexForm(QUAN17A00000002).pdf'
    'Habit Tracker.gsheet'
    'human use of animal behaviour.pdf'
     image.png
     IMG_20140914_180907579_HDR.jpg
     IMG_20140914_180911247_HDR.jpg
     IMG_20140914_180929570_HDR.jpg
     IMG_20140927_231235192.jpg
     IMG_20140928_245156855.jpg
     IMG_20141005_125809805.jpg
     IMG_20141005_130157057.jpg
     IMG_20141005_150309642.jpg
     IMG_20141005_150318826_HDR.jpg
     IMG_20141028_140243587.jpg
     IMG_20141028_142206102_HDR.jpg
     IMG_20141028_142209965_HDR.jpg
     IMG_20141028_142342414_HDR.jpg
     IMG_20141028_142350017_HDR.jpg
     IMG_20141028_142404046.jpg
     IMG_20141028_143747052.jpg
     IMG_20141028_201750255.jpg
     IMG_20141028_201754337.jpg
     IMG_20141028_204733571.jpg
     IMG_20141028_211403103.jpg
     IMG_20141028_213643757.jpg
     IMG_20141028_213701063_HDR.jpg
     IMG_20141028_213706617_HDR.jpg
     IMG_20141028_213708951.jpg
     IMG_20141028_213712885_HDR.jpg
     IMG_20141028_214034674.jpg
     IMG_20141028_214047451.jpg
     IMG_20141028_214052984.jpg
    'IMG_20141028_214426640 (1).jpg'
     IMG_20141028_214426640.jpg
    'IMG_20141028_215257925 (1).jpg'
     IMG_20141028_215257925.jpg
     IMG_20141028_215300971.jpg
    'IMG_20141028_215311168 (1).jpg'
     IMG_20141028_215311168.jpg
     IMG_20141028_215323037_HDR.jpg
     IMG_20141028_215615376.jpg
     IMG_20141028_215647694.jpg
     IMG-20180919-WA00022050d183bc59a1db9e7468a0e03c5b0a_old.jpg
     IMG_2816.JPG
     Itinerary.pdf
    'Job Hunt (1).gsheet'
    'Job Hunt (2).gsheet'
    'Job Hunt.gsheet'
    'JPEG image.jpeg'
     kaggle.json
     laptops.xlsx
    'Learning Goals.gdoc'
     LifeAnalytics.ipynb
    'Location Tracker.gsheet'
    'Mexico Travel Guide - Lonely Planet.mobi'
    'Movies To Watch.gdoc'
    'NZ Wedding List.gdoc'
    'PC Gamer 2017 Top 100 Lists Comparison (1).gsheet'
    'PC Gamer 2017 Top 100 Lists Comparison (2).gsheet'
    'PC Gamer 2017 Top 100 Lists Comparison.gsheet'
    'PC Games.gdoc'
     PEFB.gdoc
    'Personal Budget.gsheet'
     philip-gs65-stealth-9sf
     philippadimedical.pdf
    'Philip Stevens CV (1).pdf'
    'Philip Stevens CV (2).pdf'
    'Philip Stevens CV.pdf'
    'Philip Stevens - Seak Coding Exam.gdoc'
    'PNG image.png'
    'pptc153 (1).pdf'
    'Random photo dump'
    'Resume (1).gdoc'
    'Resume (2).gdoc'
    'Resume (3).gdoc'
    'Resume (4).gdoc'
     Resume.gdoc
     rhiannamedical.pdf
    'RJ and PCS Dissolution-of-marriage-jointparty.pdf'
    'Save the Date (2 of 1).jpg'
    'Save the Date (2 of 1).jpg.gdoc'
    'science club'
     Shipping.gsheet
     sm_customers.csv
     sm_customers.gsheet
     sm_items.csv
     sm_orders.csv
     sm_payments.csv
    'Spain - Andalucia 7th Edition, January 2013.epub'
     StartingList.gsheet
     StartingList.xls
    'Stock option.pdf'
    'Strength Goals (1).gsheet'
    'Strength Goals.gsheet'
     Switzerland.gsheet
    'Switzerland records.gsheet'
     Tasks.gdoc
    'TDEE 3.0 (1).xlsx.gsheet'
    'TDEE 3.0.xlsx.gsheet'
     TDEE.xlsx.gsheet
     tha-eng.zip
    'Things to Fix in Flat.gdoc'
    'Trip Budget.gsheet'
    'Untitled document (1).gdoc'
    'Untitled document (2).gdoc'
    'Untitled document.gdoc'
    'Untitled spreadsheet (10).gsheet'
    'Untitled spreadsheet (11).gsheet'
    'Untitled spreadsheet (12).gsheet'
    'Untitled spreadsheet (13).gsheet'
    'Untitled spreadsheet (1).gsheet'
    'Untitled spreadsheet (2).gsheet'
    'Untitled spreadsheet (3).gsheet'
    'Untitled spreadsheet (4).gsheet'
    'Untitled spreadsheet (5).gsheet'
    'Untitled spreadsheet (6).gsheet'
    'Untitled spreadsheet (7).gsheet'
    'Untitled spreadsheet (8).gsheet'
    'Untitled spreadsheet (9).gsheet'
    'Untitled spreadsheet.gsheet'
     VID_20141029_210755422.mp4
     VID_20141029_211013228.mp4
     VID_20141029_211217939.mp4
     ViSpot
     Voucher.pdf
     Wardrobe.gsheet
     watchlist-philipstevens-2023-11-03-06-04-utc.csv
    'Weight Tracker.gsheet'
     WIN_20221014_17_45_02_Pro.mp4



```python
!jupyter nbconvert --to markdown "/gdrive/My Drive/Colab Notebooks/Crop Disease Detection.ipynb"
```

    [NbConvertApp] WARNING | pattern '/gdrive/My Drive/Colab Notebooks/Crop Disease Detection.ipynb' matched no files
    This application is used to convert notebook files (*.ipynb)
            to various other formats.
    
            WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.
    
    Options
    =======
    The options below are convenience aliases to configurable class-options,
    as listed in the "Equivalent to" description-line of the aliases.
    To see all configurable class-options for some <cmd>, use:
        <cmd> --help-all
    
    --debug
        set log level to logging.DEBUG (maximize logging output)
        Equivalent to: [--Application.log_level=10]
    --show-config
        Show the application's configuration (human-readable format)
        Equivalent to: [--Application.show_config=True]
    --show-config-json
        Show the application's configuration (json format)
        Equivalent to: [--Application.show_config_json=True]
    --generate-config
        generate default config file
        Equivalent to: [--JupyterApp.generate_config=True]
    -y
        Answer yes to any questions instead of prompting.
        Equivalent to: [--JupyterApp.answer_yes=True]
    --execute
        Execute the notebook prior to export.
        Equivalent to: [--ExecutePreprocessor.enabled=True]
    --allow-errors
        Continue notebook execution even if one of the cells throws an error and include the error message in the cell output (the default behaviour is to abort conversion). This flag is only relevant if '--execute' was specified, too.
        Equivalent to: [--ExecutePreprocessor.allow_errors=True]
    --stdin
        read a single notebook file from stdin. Write the resulting notebook with default basename 'notebook.*'
        Equivalent to: [--NbConvertApp.from_stdin=True]
    --stdout
        Write notebook output to stdout instead of files.
        Equivalent to: [--NbConvertApp.writer_class=StdoutWriter]
    --inplace
        Run nbconvert in place, overwriting the existing notebook (only
                relevant when converting to notebook format)
        Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory=]
    --clear-output
        Clear output of current file and save in place,
                overwriting the existing notebook.
        Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --ClearOutputPreprocessor.enabled=True]
    --no-prompt
        Exclude input and output prompts from converted document.
        Equivalent to: [--TemplateExporter.exclude_input_prompt=True --TemplateExporter.exclude_output_prompt=True]
    --no-input
        Exclude input cells and output prompts from converted document.
                This mode is ideal for generating code-free reports.
        Equivalent to: [--TemplateExporter.exclude_output_prompt=True --TemplateExporter.exclude_input=True --TemplateExporter.exclude_input_prompt=True]
    --allow-chromium-download
        Whether to allow downloading chromium if no suitable version is found on the system.
        Equivalent to: [--WebPDFExporter.allow_chromium_download=True]
    --disable-chromium-sandbox
        Disable chromium security sandbox when converting to PDF..
        Equivalent to: [--WebPDFExporter.disable_sandbox=True]
    --show-input
        Shows code input. This flag is only useful for dejavu users.
        Equivalent to: [--TemplateExporter.exclude_input=False]
    --embed-images
        Embed the images as base64 dataurls in the output. This flag is only useful for the HTML/WebPDF/Slides exports.
        Equivalent to: [--HTMLExporter.embed_images=True]
    --sanitize-html
        Whether the HTML in Markdown cells and cell outputs should be sanitized..
        Equivalent to: [--HTMLExporter.sanitize_html=True]
    --log-level=<Enum>
        Set the log level by value or name.
        Choices: any of [0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL']
        Default: 30
        Equivalent to: [--Application.log_level]
    --config=<Unicode>
        Full path of a config file.
        Default: ''
        Equivalent to: [--JupyterApp.config_file]
    --to=<Unicode>
        The export format to be used, either one of the built-in formats
                ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'rst', 'script', 'slides', 'webpdf']
                or a dotted object name that represents the import path for an
                ``Exporter`` class
        Default: ''
        Equivalent to: [--NbConvertApp.export_format]
    --template=<Unicode>
        Name of the template to use
        Default: ''
        Equivalent to: [--TemplateExporter.template_name]
    --template-file=<Unicode>
        Name of the template file to use
        Default: None
        Equivalent to: [--TemplateExporter.template_file]
    --theme=<Unicode>
        Template specific theme(e.g. the name of a JupyterLab CSS theme distributed
        as prebuilt extension for the lab template)
        Default: 'light'
        Equivalent to: [--HTMLExporter.theme]
    --sanitize_html=<Bool>
        Whether the HTML in Markdown cells and cell outputs should be sanitized.This
        should be set to True by nbviewer or similar tools.
        Default: False
        Equivalent to: [--HTMLExporter.sanitize_html]
    --writer=<DottedObjectName>
        Writer class used to write the
                                            results of the conversion
        Default: 'FilesWriter'
        Equivalent to: [--NbConvertApp.writer_class]
    --post=<DottedOrNone>
        PostProcessor class used to write the
                                            results of the conversion
        Default: ''
        Equivalent to: [--NbConvertApp.postprocessor_class]
    --output=<Unicode>
        overwrite base name use for output files.
                    can only be used when converting one notebook at a time.
        Default: ''
        Equivalent to: [--NbConvertApp.output_base]
    --output-dir=<Unicode>
        Directory to write output(s) to. Defaults
                                      to output to the directory of each notebook. To recover
                                      previous default behaviour (outputting to the current
                                      working directory) use . as the flag value.
        Default: ''
        Equivalent to: [--FilesWriter.build_directory]
    --reveal-prefix=<Unicode>
        The URL prefix for reveal.js (version 3.x).
                This defaults to the reveal CDN, but can be any url pointing to a copy
                of reveal.js.
                For speaker notes to work, this must be a relative path to a local
                copy of reveal.js: e.g., "reveal.js".
                If a relative path is given, it must be a subdirectory of the
                current directory (from which the server is run).
                See the usage documentation
                (https://nbconvert.readthedocs.io/en/latest/usage.html#reveal-js-html-slideshow)
                for more details.
        Default: ''
        Equivalent to: [--SlidesExporter.reveal_url_prefix]
    --nbformat=<Enum>
        The nbformat version to write.
                Use this to downgrade notebooks.
        Choices: any of [1, 2, 3, 4]
        Default: 4
        Equivalent to: [--NotebookExporter.nbformat_version]
    
    Examples
    --------
    
        The simplest way to use nbconvert is
    
                > jupyter nbconvert mynotebook.ipynb --to html
    
                Options include ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'rst', 'script', 'slides', 'webpdf'].
    
                > jupyter nbconvert --to latex mynotebook.ipynb
    
                Both HTML and LaTeX support multiple output templates. LaTeX includes
                'base', 'article' and 'report'.  HTML includes 'basic', 'lab' and
                'classic'. You can specify the flavor of the format used.
    
                > jupyter nbconvert --to html --template lab mynotebook.ipynb
    
                You can also pipe the output to stdout, rather than a file
    
                > jupyter nbconvert mynotebook.ipynb --stdout
    
                PDF is generated via latex
    
                > jupyter nbconvert mynotebook.ipynb --to pdf
    
                You can get (and serve) a Reveal.js-powered slideshow
    
                > jupyter nbconvert myslides.ipynb --to slides --post serve
    
                Multiple notebooks can be given at the command line in a couple of
                different ways:
    
                > jupyter nbconvert notebook*.ipynb
                > jupyter nbconvert notebook1.ipynb notebook2.ipynb
    
                or you can specify the notebooks list in a config file, containing::
    
                    c.NbConvertApp.notebooks = ["my_notebook.ipynb"]
    
                > jupyter nbconvert --config mycfg.py
    
    To see all available configurables, use `--help-all`.
    


Detecting crop diseases early is crucial for food security and getting the most out of our harvests. My goal is to create a model that can spot these diseases accurately just by looking at images of leaves.

To do this, I'm turning to the [PlantVillage](https://www.plantvillage.org) dataset. It's got over 54,000 images showing both healthy and sick leaves, sorted into 38 groups based on what kind of plant it is and what disease it might have. You can find this dataset in the [TensorFlow Datasets catalog](https://www.tensorflow.org/datasets/catalog/plant_village).

I'm going to try two different approaches using Keras: first, I'll build a custom Convolutional Neural Network from scratch, and then I'll fine-tune an EfficientNet model.

# Setup


```python
#Remove from markdown
from google.colab import drive
drive.mount('/content/gdrive')
```

    Mounted at /content/gdrive



```python
!pip install -q --upgrade tensorflow
!pip install -q --upgrade keras
```

    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m589.8/589.8 MB[0m [31m1.6 MB/s[0m eta [36m0:00:00[0m
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m5.3/5.3 MB[0m [31m103.5 MB/s[0m eta [36m0:00:00[0m
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m2.2/2.2 MB[0m [31m88.5 MB/s[0m eta [36m0:00:00[0m
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m5.5/5.5 MB[0m [31m100.0 MB/s[0m eta [36m0:00:00[0m
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m1.1/1.1 MB[0m [31m73.6 MB/s[0m eta [36m0:00:00[0m
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m311.2/311.2 kB[0m [31m38.6 MB/s[0m eta [36m0:00:00[0m
    [?25h[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
    tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.16.1 which is incompatible.[0m[31m
    [0m


```python
import os

import keras
import matplotlib.pyplot as plt
import numpy as np
import tensorflow_datasets as tfds

from keras import layers
from keras.applications import EfficientNetB0
from sklearn.metrics import classification_report
```

# Data

I'll split the data into three parts: 80% for training, 10% for validation, and 10% for testing. This ensures I've got solid sets for each step. I'll batch and shuffle the data, getting it ready for efficient processing and stopping the model from memorizing the order of samples. During training, the dataset will prefetch by default, keeping the flow smooth and minimizing downtime to make the most of the model training runs.

On top of that, I'll pull out the class names for easy reference later. The labels will be encoded as integers in the same order as the list, making it simple to connect with other parts of my setup.


```python
BATCH_SIZE = 128

(train, val, test), metadata = tfds.load(
    'plant_village',
    split=['train[:80%]','train[80%:90%]','train[90%:]'],
    batch_size=BATCH_SIZE,
    shuffle_files=True,
    as_supervised=True,
    with_info=True,
)

NUM_CLASSES = metadata.features['label'].num_classes
CLASS_NAMES = metadata.features['label'].names
```

    Downloading and preparing dataset 827.82 MiB (download: 827.82 MiB, generated: 815.37 MiB, total: 1.60 GiB) to /root/tensorflow_datasets/plant_village/1.0.2...



    Dl Completed...: 0 url [00:00, ? url/s]



    Dl Size...: 0 MiB [00:00, ? MiB/s]



    Extraction completed...: 0 file [00:00, ? file/s]


    IOPub message rate exceeded.
    The notebook server will temporarily stop sending output
    to the client in order to avoid crashing it.
    To change this limit, set the config variable
    `--NotebookApp.iopub_msg_rate_limit`.
    
    Current values:
    NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
    NotebookApp.rate_limit_window=3.0 (secs)
    
    IOPub message rate exceeded.
    The notebook server will temporarily stop sending output
    to the client in order to avoid crashing it.
    To change this limit, set the config variable
    `--NotebookApp.iopub_msg_rate_limit`.
    
    Current values:
    NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
    NotebookApp.rate_limit_window=3.0 (secs)
    
    IOPub message rate exceeded.
    The notebook server will temporarily stop sending output
    to the client in order to avoid crashing it.
    To change this limit, set the config variable
    `--NotebookApp.iopub_msg_rate_limit`.
    
    Current values:
    NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
    NotebookApp.rate_limit_window=3.0 (secs)
    
    IOPub message rate exceeded.
    The notebook server will temporarily stop sending output
    to the client in order to avoid crashing it.
    To change this limit, set the config variable
    `--NotebookApp.iopub_msg_rate_limit`.
    
    Current values:
    NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
    NotebookApp.rate_limit_window=3.0 (secs)
    



    Generating splits...:   0%|          | 0/1 [00:00<?, ? splits/s]



    Generating train examples...:   0%|          | 0/54303 [00:00<?, ? examples/s]



    Shuffling /root/tensorflow_datasets/plant_village/1.0.2.incompleteW4QO52/plant_village-train.tfrecord*...:   0…


    Dataset plant_village downloaded and prepared to /root/tensorflow_datasets/plant_village/1.0.2. Subsequent calls will reuse this data.



```python
plt.figure(figsize=(10, 10))
for images, labels in train.take(1):
    for i in range(9):
        ax = plt.subplot(3, 3, i + 1)
        plt.imshow(np.array(images[i]).astype("uint8")) #Read from dict
        plt.title(CLASS_NAMES[int(labels[i])], fontsize=10) #Lower font size
        plt.axis("off")
```


    
![png](Crop%20Disease%20Detection_files/Crop%20Disease%20Detection_9_0.png)
    


# Custom Model



## Build
To start, I’ll craft a custom CNN model using the Keras Functional API. Here's a detailed breakdown of the architecture:

1. **Input and Preprocessing**: It takes the images as input, which are expected to have dimensions of 256x256 pixels and 3 color channels (RGB). The raw images from the dataset come like this. The input images then undergo some data augmentation, which includes transformations like rotation, flipping, translation and adjustments to contrast to increase the diversity of the training data. I also normalize the images so that the pixel values of the images are scaled down to a range between 0 and 1.
2. **Feature Extraction**: The initial convolutional layer operates on normalized images, employing 128 filters sized at 3x3 pixels with a stride of 2 pixels. Batch normalization and ReLU activation are applied here and after most convolutional layers that follow.
    
    Following this, there are three blocks each with three convolutional layers. The number of feature maps in each block doubles compared to the previous block. Within each block, two consecutive layers use separable convolution for speed and efficiency, followed by max-pooling to reduce the feature map size by half. Simultaneously, there's another convolutional layer running in parallel. Its output connects with a residual connection, which adds its result to the output of the max-pooling layer. This combined output is then forwarded to the next block.
    
    One final convolutional layer is added to the output of the three blocks concluding in 1024 16x16 feature maps.
    
3. **Output**: After the feature extraction, global average pooling is applied to reduce the spatial dimensions of the feature maps to a single vector with 1024 dimensions for each image. Dropout is applied to the pooled features to prevent overfitting by randomly setting a fraction of input units to 0 at each update during training. Finally, a dense layer produces the output logits, which represent the unnormalized scores for each of the 38 classes. No activation function is applied to this layer, as it aims to return raw logit scores rather than probabilities.


```python
data_augmentation_layers = [
    layers.RandomRotation(factor=0.15),
    layers.RandomTranslation(height_factor=0.1, width_factor=0.1),
    layers.RandomFlip(),
    layers.RandomContrast(factor=0.1),
]

def data_augmentation(images):
    for layer in data_augmentation_layers:
        images = layer(images)
    return images

def build_custom_model(num_classes):
    # Input
    inputs = keras.Input(shape=(256, 256,3))
    x = data_augmentation(inputs)
    x = layers.Rescaling(1.0 / 255)(x)

    # Feature Extraction
    x = layers.Conv2D(128, 3, strides=2, padding="same")(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation("relu")(x)
    previous_block_activation = x

    for size in [256, 512, 728]:
        x = layers.Activation("relu")(x)

        x = layers.SeparableConv2D(size, 3, padding="same")(x)
        x = layers.BatchNormalization()(x)
        x = layers.Activation("relu")(x)

        x = layers.SeparableConv2D(size, 3, padding="same")(x)
        x = layers.BatchNormalization()(x)

        x = layers.MaxPooling2D(3, strides=2, padding="same")(x)

        # Residual
        residual = layers.Conv2D(size, 1, strides=2, padding="same")(
            previous_block_activation
        )
        x = layers.add([x, residual])
        previous_block_activation = x

    x = layers.SeparableConv2D(1024, 3, padding="same")(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation("relu")(x)

    # Output
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dropout(0.25)(x)
    outputs = layers.Dense(num_classes, activation=None)(x)

    return keras.Model(inputs, outputs, name="CustomModel")


custom_model = build_custom_model(num_classes=NUM_CLASSES)
keras.utils.plot_model(custom_model, show_shapes=True)
```




    
![png](Crop%20Disease%20Detection_files/Crop%20Disease%20Detection_12_0.png)
    



## Train

In this phase, I train the custom model employing the Adam optimizer with a learning rate set to 0.0003. For training, I employ sparse categorical cross-entropy loss and track the accuracy metric. Throughout training, I save model checkpoints at the conclusion of each epoch. The training spans 25 epochs utilizing the training data, with validation data employed for validation throughout. The training history is then returned, serving as the basis for plotting the training run.


```python
callbacks = [
    keras.callbacks.ModelCheckpoint("custom_model_{epoch:02d}-{val_loss:.2f}.keras"),
]
custom_model.compile(
    optimizer=keras.optimizers.Adam(3e-4),
    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=[keras.metrics.SparseCategoricalAccuracy(name="accuracy")],
)
hist = custom_model.fit(
    train,
    epochs=25,
    callbacks=callbacks,
    validation_data=val,
)
```

    Epoch 1/25
    [1m340/340[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m217s[0m 560ms/step - accuracy: 0.6436 - loss: 1.3229 - val_accuracy: 0.1007 - val_loss: 3.7074
    Epoch 2/25
    [1m340/340[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m182s[0m 537ms/step - accuracy: 0.9103 - loss: 0.3222 - val_accuracy: 0.3305 - val_loss: 3.0962
    Epoch 3/25
    [1m340/340[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m182s[0m 537ms/step - accuracy: 0.9501 - loss: 0.1772 - val_accuracy: 0.6875 - val_loss: 1.0931
    Epoch 4/25
    [1m340/340[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m182s[0m 537ms/step - accuracy: 0.9654 - loss: 0.1206 - val_accuracy: 0.7512 - val_loss: 0.9421
    Epoch 5/25
    [1m340/340[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m182s[0m 537ms/step - accuracy: 0.9749 - loss: 0.0865 - val_accuracy: 0.8634 - val_loss: 0.4647
    Epoch 6/25
    [1m340/340[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m182s[0m 537ms/step - accuracy: 0.9787 - loss: 0.0715 - val_accuracy: 0.8177 - val_loss: 0.6186
    Epoch 7/25
    [1m340/340[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m182s[0m 537ms/step - accuracy: 0.9836 - loss: 0.0568 - val_accuracy: 0.8748 - val_loss: 0.4183
    Epoch 8/25
    [1m340/340[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m182s[0m 537ms/step - accuracy: 0.9839 - loss: 0.0555 - val_accuracy: 0.8273 - val_loss: 0.6005
    Epoch 9/25
    [1m340/340[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m182s[0m 536ms/step - accuracy: 0.9844 - loss: 0.0509 - val_accuracy: 0.9157 - val_loss: 0.2416
    Epoch 10/25
    [1m340/340[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m182s[0m 536ms/step - accuracy: 0.9883 - loss: 0.0385 - val_accuracy: 0.8776 - val_loss: 0.3853
    Epoch 11/25
    [1m340/340[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m182s[0m 537ms/step - accuracy: 0.9885 - loss: 0.0382 - val_accuracy: 0.8904 - val_loss: 0.3542
    Epoch 12/25
    [1m340/340[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m182s[0m 537ms/step - accuracy: 0.9885 - loss: 0.0362 - val_accuracy: 0.9072 - val_loss: 0.2743
    Epoch 13/25
    [1m340/340[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m182s[0m 536ms/step - accuracy: 0.9895 - loss: 0.0351 - val_accuracy: 0.8919 - val_loss: 0.3177
    Epoch 14/25
    [1m340/340[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m182s[0m 537ms/step - accuracy: 0.9920 - loss: 0.0280 - val_accuracy: 0.9556 - val_loss: 0.1271
    Epoch 15/25
    [1m340/340[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m182s[0m 536ms/step - accuracy: 0.9910 - loss: 0.0294 - val_accuracy: 0.9184 - val_loss: 0.2643
    Epoch 16/25
    [1m340/340[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m182s[0m 537ms/step - accuracy: 0.9902 - loss: 0.0334 - val_accuracy: 0.9356 - val_loss: 0.1837
    Epoch 17/25
    [1m340/340[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m183s[0m 537ms/step - accuracy: 0.9922 - loss: 0.0250 - val_accuracy: 0.7881 - val_loss: 0.8638
    Epoch 18/25
    [1m340/340[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m182s[0m 537ms/step - accuracy: 0.9923 - loss: 0.0258 - val_accuracy: 0.9774 - val_loss: 0.0707
    Epoch 19/25
    [1m340/340[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m182s[0m 537ms/step - accuracy: 0.9924 - loss: 0.0243 - val_accuracy: 0.9321 - val_loss: 0.2613
    Epoch 20/25
    [1m340/340[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m182s[0m 537ms/step - accuracy: 0.9904 - loss: 0.0291 - val_accuracy: 0.9549 - val_loss: 0.1336
    Epoch 21/25
    [1m340/340[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m182s[0m 537ms/step - accuracy: 0.9922 - loss: 0.0243 - val_accuracy: 0.7805 - val_loss: 0.8193
    Epoch 22/25
    [1m340/340[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m182s[0m 537ms/step - accuracy: 0.9942 - loss: 0.0201 - val_accuracy: 0.9181 - val_loss: 0.2905
    Epoch 23/25
    [1m340/340[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m183s[0m 537ms/step - accuracy: 0.9945 - loss: 0.0175 - val_accuracy: 0.9705 - val_loss: 0.0921
    Epoch 24/25
    [1m340/340[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m182s[0m 537ms/step - accuracy: 0.9934 - loss: 0.0199 - val_accuracy: 0.9464 - val_loss: 0.1890
    Epoch 25/25
    [1m340/340[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m182s[0m 537ms/step - accuracy: 0.9923 - loss: 0.0237 - val_accuracy: 0.9291 - val_loss: 0.2638



```python
# Remove from markdown
custom_model.save("/content/gdrive/MyDrive/crop_model4.keras")
```

## Evaluate
In this section, I visualize the model training and examine the classification report from sklearn, featuring metrics like precision, recall, F1-score, and support for each class. It also includes overall average and weighted average metrics for the per-class statistics.

Upon review, a couple of observations stand out:

1. The training seems to converge rapidly, yet there's significant jitteriness in the validation curve likely due to small batch size and high learning rate.
2. Overall accuracy on the test set looks solid at 93%, though we observe that certain classes with lower support exhibit less impressive precision or recall.


```python
# Remove from markdown
custom_model = keras.saving.load_model("/content/gdrive/MyDrive/crop_model4.keras")
```


```python
def plot_hist(hist):
    plt.plot(hist.history["accuracy"])
    plt.plot(hist.history["val_accuracy"])
    plt.title("model accuracy")
    plt.ylabel("accuracy")
    plt.xlabel("epoch")
    plt.legend(["train", "validation"], loc="upper left")
    plt.show()

def print_report(model, data):
  pred = np.argmax(model.predict(data), axis=-1)
  y = np.concatenate([y for x, y in data], axis=0)
  report = classification_report(y, pred, target_names = CLASS_NAMES)
  print(report)
```


```python
plot_hist(hist)
```


    
![png](Crop%20Disease%20Detection_files/Crop%20Disease%20Detection_19_0.png)
    



```python
print_report(custom_model, test)
```

    [1m43/43[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m6s[0m 130ms/step
                                                   precision    recall  f1-score   support
    
                               Apple___Apple_scab       1.00      0.88      0.94        58
                                Apple___Black_rot       1.00      0.91      0.95        65
                         Apple___Cedar_apple_rust       1.00      1.00      1.00        25
                                  Apple___healthy       1.00      0.84      0.91       159
                              Blueberry___healthy       1.00      0.99      1.00       133
                                 Cherry___healthy       1.00      0.32      0.49        90
                          Cherry___Powdery_mildew       0.99      0.97      0.98       100
       Corn___Cercospora_leaf_spot Gray_leaf_spot       0.87      0.96      0.92        56
                               Corn___Common_rust       1.00      1.00      1.00       127
                                   Corn___healthy       1.00      1.00      1.00       112
                      Corn___Northern_Leaf_Blight       0.96      0.94      0.95       112
                                Grape___Black_rot       0.97      0.96      0.96       112
                     Grape___Esca_(Black_Measles)       1.00      0.96      0.98       126
                                  Grape___healthy       0.97      1.00      0.99        36
       Grape___Leaf_blight_(Isariopsis_Leaf_Spot)       1.00      0.83      0.91       105
         Orange___Haunglongbing_(Citrus_greening)       1.00      1.00      1.00       561
                           Peach___Bacterial_spot       0.91      0.98      0.95       237
                                  Peach___healthy       0.45      1.00      0.62        27
                    Pepper,_bell___Bacterial_spot       0.97      0.98      0.98       105
                           Pepper,_bell___healthy       0.76      0.99      0.86       154
                            Potato___Early_blight       0.70      1.00      0.82        98
                                 Potato___healthy       0.89      0.80      0.84        10
                             Potato___Late_blight       0.88      0.95      0.92       104
                              Raspberry___healthy       1.00      0.90      0.95        30
                                Soybean___healthy       0.95      1.00      0.97       527
                          Squash___Powdery_mildew       0.92      1.00      0.96       188
                             Strawberry___healthy       1.00      0.76      0.86        50
                         Strawberry___Leaf_scorch       1.00      0.90      0.95        88
                          Tomato___Bacterial_spot       0.95      0.97      0.96       184
                            Tomato___Early_blight       0.74      0.97      0.84        96
                                 Tomato___healthy       0.98      1.00      0.99       164
                             Tomato___Late_blight       0.99      0.75      0.86       191
                               Tomato___Leaf_Mold       1.00      0.57      0.72        97
                      Tomato___Septoria_leaf_spot       0.95      0.85      0.89       181
    Tomato___Spider_mites Two-spotted_spider_mite       0.72      0.99      0.83       168
                             Tomato___Target_Spot       0.90      0.94      0.92       149
                     Tomato___Tomato_mosaic_virus       0.97      0.97      0.97        36
           Tomato___Tomato_Yellow_Leaf_Curl_Virus       1.00      0.93      0.96       569
    
                                         accuracy                           0.93      5430
                                        macro avg       0.93      0.91      0.91      5430
                                     weighted avg       0.95      0.93      0.93      5430
    


# Fine-tune EfficientNet

Rather than using a custom model, let's finetune a pretrained EfficientNet model.

## Build
Here I construct my model by harnessing the power of the EfficientNet architecture along with pre-trained weights. Here's a detailed breakdown of the process:

1. **Input and Preprocessing**:  The variant of EfficientNet I'm utilizing expects images with dimensions of 224x224 pixels. Therefore, I ensure that the images are resized accordingly to meet this requirement. The input also undergoes the same data augmentation as the custom model did. No normalizing is done here as the EfficientNet already handles all other aspects of preprocessing.
2. **EfficientNetB0:** I employ the EfficientNetB0 model that's been pre-trained on the ImageNet dataset. Since we won't be using the top layer for ImageNet classes, I discard it. Additionally, I freeze the weights to preserve the learned representations, keeping them static during the initial training phases.
3. **Output**: To tailor the model for our specific classification task, I introduce additional layers atop the pre-trained base. This includes incorporating global average pooling, batch normalization, dropout, and a dense layer devoid of an activation function, which yields the class predictions.



```python
def build_efficientnet_model(num_classes):

    # Input
    inputs = layers.Input(shape=(256, 256, 3))
    x = data_augmentation(inputs)
    x = layers.Resizing(224, 224)(x)

    # Pretrained model with frozen weights and top layer removed
    model = EfficientNetB0(include_top=False, input_tensor=x, weights="imagenet")
    model.trainable = False

    # Output
    x = layers.GlobalAveragePooling2D()(model.output)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.2)(x)
    outputs = layers.Dense(num_classes, activation=None)(x)

    return keras.Model(inputs, outputs, name="EfficientNet")

en_model = build_efficientnet_model(num_classes=NUM_CLASSES)
keras.utils.plot_model(en_model)
```

## Train
First, I'll train the custom top layer of this model using a relatively high learning rate to quickly get a decent model. Then, I'll unfreeze the EfficientNetB0 layers and train again with a smaller learning rate. This method allows for only subtle adjustments to be made to the pre-trained weights so we don't lose any critical feature representations in the initial stages of training. The idea is to keep the underlying power of the pretrained model and make it work better for our specific needs.

### Step 1


```python
callbacks = [
    keras.callbacks.ModelCheckpoint("efficientnet_model_step_1_{epoch:02d}-{val_loss:.2f}.keras"),
]

en_model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-2),
    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=[keras.metrics.SparseCategoricalAccuracy(name="accuracy")],
)

step1_hist = en_model.fit(
    train,
    epochs=25,
    callbacks=callbacks,
    validation_data=val,
)
```

### Step 2


```python
def unfreeze_model(model):
    # We unfreeze the top 20 layers while leaving BatchNorm layers frozen
    for layer in model.layers[-20:]:
        if not isinstance(layer, layers.BatchNormalization):
            layer.trainable = True

unfreeze_model(en_model)
```


```python
callbacks = [
    keras.callbacks.ModelCheckpoint("efficientnet_model_step_2_{epoch:02d}-{val_loss:.2f}.keras"),
]

en_model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-5),
    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=[keras.metrics.SparseCategoricalAccuracy(name="accuracy")]
)

step2_hist = en_model.fit(
    train,
    epochs=4,
    callbacks=callbacks,
    validation_data=val)
```


```python
#Remove from markdown
en_model.save("/content/gdrive/MyDrive/en_crop_model4.keras")
```

## Evaluate
As before, I visualize the model training and examine the per-class and overall classification metrics.

Here, a few observations stand out:

1. The training run reaches high accuracy even before I unfreeze the bottom pre-trained layers. Again there is some jitteriness in the validation curve in stage 1 likely due to small batch size and high learning rate. However, further training these layers results in a slight enhancement in overall accuracy and a more robust model.
2. The validation accuracy is higher than the training accuracy. This is common behaviour when using dropout layers, as they are only used in training and deliberately hamper the model.
2. Overall accuracy on the test set impressively reaches 97%, with all classes exhibiting high precision and recall. Nonetheless, some classes still demonstrate slightly better performance than others.








```python
plot_hist(step1_hist)
```


    
![png](Crop%20Disease%20Detection_files/Crop%20Disease%20Detection_32_0.png)
    



```python
plot_hist(step2_hist)
```


    
![png](Crop%20Disease%20Detection_files/Crop%20Disease%20Detection_33_0.png)
    



```python
print_report(en_model, test)
```

    [1m43/43[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m2s[0m 48ms/step
                                                   precision    recall  f1-score   support
    
                               Apple___Apple_scab       0.96      0.93      0.95        58
                                Apple___Black_rot       1.00      1.00      1.00        65
                         Apple___Cedar_apple_rust       1.00      0.88      0.94        25
                                  Apple___healthy       0.98      0.99      0.99       159
                              Blueberry___healthy       0.99      1.00      1.00       133
                                 Cherry___healthy       1.00      0.99      0.99        90
                          Cherry___Powdery_mildew       0.99      1.00      1.00       100
       Corn___Cercospora_leaf_spot Gray_leaf_spot       0.92      0.88      0.90        56
                               Corn___Common_rust       0.98      1.00      0.99       127
                                   Corn___healthy       1.00      0.99      1.00       112
                      Corn___Northern_Leaf_Blight       0.94      0.96      0.95       112
                                Grape___Black_rot       0.96      0.96      0.96       112
                     Grape___Esca_(Black_Measles)       0.98      0.97      0.97       126
                                  Grape___healthy       1.00      1.00      1.00        36
       Grape___Leaf_blight_(Isariopsis_Leaf_Spot)       1.00      1.00      1.00       105
         Orange___Haunglongbing_(Citrus_greening)       1.00      1.00      1.00       561
                           Peach___Bacterial_spot       1.00      0.99      0.99       237
                                  Peach___healthy       0.93      1.00      0.96        27
                    Pepper,_bell___Bacterial_spot       0.99      0.98      0.99       105
                           Pepper,_bell___healthy       0.99      1.00      0.99       154
                            Potato___Early_blight       0.96      0.99      0.97        98
                                 Potato___healthy       1.00      0.80      0.89        10
                             Potato___Late_blight       0.97      0.96      0.97       104
                              Raspberry___healthy       1.00      1.00      1.00        30
                                Soybean___healthy       0.99      1.00      1.00       527
                          Squash___Powdery_mildew       1.00      1.00      1.00       188
                             Strawberry___healthy       0.98      1.00      0.99        50
                         Strawberry___Leaf_scorch       1.00      0.98      0.99        88
                          Tomato___Bacterial_spot       0.94      0.93      0.94       184
                            Tomato___Early_blight       0.86      0.66      0.75        96
                                 Tomato___healthy       0.96      0.99      0.98       164
                             Tomato___Late_blight       0.87      0.95      0.91       191
                               Tomato___Leaf_Mold       0.96      0.84      0.90        97
                      Tomato___Septoria_leaf_spot       0.94      0.88      0.91       181
    Tomato___Spider_mites Two-spotted_spider_mite       0.90      0.96      0.93       168
                             Tomato___Target_Spot       0.83      0.95      0.89       149
                     Tomato___Tomato_mosaic_virus       0.97      0.97      0.97        36
           Tomato___Tomato_Yellow_Leaf_Curl_Virus       0.99      0.98      0.98       569
    
                                         accuracy                           0.97      5430
                                        macro avg       0.97      0.96      0.96      5430
                                     weighted avg       0.97      0.97      0.97      5430
    

